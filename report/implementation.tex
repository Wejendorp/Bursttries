\chapter{Implementation}
By first establishing the regular trie methods, a basis is made for evaluating
the modifications required by the other types.

\section{Generic Trie}

\begin{algorithm}[H]
    \caption{\FuncSty{Insert(}$k$\FuncSty{)}}
    $Node \leftarrow$  root node\;
    \ForEach{char $c$ in $k$}{
        \eIf{$Node$ has child $c$}{
            $Node \leftarrow Node[c]$\;
        }{ %else
            \emph{Create $Node[c]$}\;
            $Node \leftarrow Node[c]$\;
        }
    }
    \emph{Set end of string flag on $Node$}\;
\end{algorithm}
Using this as the basis for the insertion and deletion methods, the trie is
built and searched by reading one character at a time. So just like an
insertion might create a deep path down the trie, a deletion will have to
remove dead ends, upon removing an entry.

A depth-first approach is taken with deletions. The recursive call returns with
a value, determining if changes might be needed at the higher level.


With the nodes having $\alpha$ pointers, the memory overhead for sparse tries
is very big.

As a function of collision odds, each unique subtrie incurs $(\alpha-1) l$
pointers of overhead for a unique substring of length $l$. As such, the total
pointer count is anywhere up to:
\begin{align*}
    M(n) = O(n * (\alpha l - 1))\\
\end{align*}
Which will be used as baseline for comparisons.

The cost of insertion is a function of the chance that a string is unique on
each level. The first insertion has 0 chance of being non-unique, creating a
base-case costing $\alpha$ new pointers in the created subnode.
\[
    T(n) = \left\{ \begin{array}{ll}
                    (\frac{\alpha - 1}{\alpha})^m           & \text{if } n = 1 \\
                    T(n-1) + (\frac{\alpha - 1}{\alpha})^m   & \text{if } n > 1
                   \end{array}
                \right.
\]
For the m'th insertion of an n-length word to the node.




\section{Burst trie}
Based on the descriptions in \cite{Nash2008}, the algorithms for search
and insertion are as such:

\begin{algorithm}[H]
    \caption{\FuncSty{Search(}$k$\FuncSty{)}}
    \label{alg:bt_search}

    $Node \leftarrow$  root node\;
    \ForEach{char $c$ in $k$}{
        \eIf{$Node$ has child $c$}{
            $Node \leftarrow Node[c]$\;
        }{ %else
            \eIf{$Node$ has bucket $B[c]$}{
                \emph{Lookup remaining $k$ in $B[c]$}\;
            }{
                \Return{Nothing}\;
            }
        }
    }
\end{algorithm}

where the lookup phase depends on the bucket structure chosen. The original
lookup uses sorted doubly-linked lists. 

\begin{algorithm}[H]
    \caption{\FuncSty{Insert(}$k$\FuncSty{)}}
    \label{alg:bt_insert}

    \SetKwFunction{Burst}{burst}

    $Node \leftarrow$  root node\;
    \tcp{Move down until reaching bucket reference}
    \ForEach{char $c$ in $k$}{
        \If{$Node$ has child $c$}{
            $Node \leftarrow Node[c]$\;
        }
    }
    \tcp{Node either has a bucket at $c$ or one is created}
    \eIf{$Node$ has bucket $B[c]$}{
        \emph{Insert remaining $k$ in $B$}\;
        \If{$B$ is full}{
            $Node[c] \leftarrow \Burst(B)$\;
        }
    }{
        $b \leftarrow $\emph{Create bucket and insert $k$}\;
    }
\end{algorithm}

Deletions are mentioned to be equivalent to insertions, in that removing a key
may cause removal of a bucket, and nodes. The method being equivalent to that
of the basic trie, or the insertion algorithm in reverse.

\begin{algorithm}[H]
    \caption{\FuncSty{Burst(}$b$\FuncSty{)}}
    \label{alg:bt_burst}

    \ForEach{value $k,v$ in $b$}{
        \If{$Node$ has child $c$}{
            $Node \leftarrow Node[c]$\;
        }
    }
\end{algorithm}


\subsection{Bucket structures}
% Advantages
% Disadvantages
% Time bounds
Depending on the chosen bucket structure, insertions, deletions and searches
accomplish different time complexities, and allow different bucket sizes
without becoming inefficient.
An added dimension to this research when compared to previous attempts is the
changes needed to make each container threadsafe.

\subsubsection{Linkedlists}
% Advantages
Easy to maintain. Constant time insertions.
If the linkedlist is maintained with a {\keyword head} and {\keyword tail} pointer,
and kept single-linked, insertions using a simple {\keyword AO\_compare\_and\_swap}
pointer-swap.

% Disadvantages
A linked-list is known for its bad cache locality, which makes it slow in
searches and iterations when compared to an array implementation, despite
having the same time bounds. A linkedlist is a pointer-based bucket, making
random-acccess impossible, resulting in limited options for searching and
sorting. Assuming a worst-case allocation scenario, a traversal of the bucket
incurs up to $b$ cache misses in a per-node allocation scheme.

% Space consumption
The space consumption of linkedlists are equivalent to that of a binary tree,
if a doubly-linked list is used, or one pointer less per entry for a
single-linked list.

% Time bounds
\begin{table}[h!]
    \centering
    \begin{tabular}[here]{ r || c | c | c}
        Operation & Average case & Worst case & Requires locking  \\ \hline
        Insert    & $O(1)$ & $O(1)$   & C  \\ \hline
        Delete    & $O(n)$ & $O(n)$   & E \\ \hline
        Find      & $O(n)$ & $O(n)$   & C  \\ \hline
    \end{tabular}
    \caption{Time bounds for the bucket operations using unsorted linkedlists,
    $n$ denoting number of elements in the list. Locking terms use E for
        exclusive and C for concurrent.}
    \label{tab:bounds:linkedlist}
\end{table}
The find and insert operations can be kept separate from delete by using a
read-lock.

\subsubsection{Binary search tree}
% Advantages
The binary search trees allow for a (average case) logarithmic insertion,
search and deletion structure. This was found to be the optimal
bounds\cite{Nash2008}.

So the binary search trees are theoretically optimal, while having little space
overhead.

% Disadvantages
However, the binary search trees were found to be inefficient due to caching,
being a pointer-based bucket structure. A basic binary search tree has no
guarantees on the height, which is based on the insertion order. This doing a
pre-order tree-walk when bursting, preserving the insertion order, on the next
part of the key.

%Space consumption
The binary search trees will only use 2 pointers per element, and
has no overhead from preallocation. 

% Time bounds
\begin{table}[h!]
    \centering
    \begin{tabular}[here]{ l || c | c | c}
        Operation & Average case & Worst case & Requires locking  \\ \hline
        Insert    & $O(\log n)$ & $O(n)$ & E \\ \hline
        Delete    & $O(\log n)$ & $O(n)$ & E \\ \hline
        Find      & $O(\log n)$ & $O(n)$ & C  \\ \hline
    \end{tabular}
    \caption{Time bounds for the bucket operations using binary search trees,
    $n$ denoting number of elements in the bucket. Locking terms use E for
        exclusive and C for concurrent.}
    \label{tab:bounds:linkedlist}
\end{table}
The find and insert operations can be kept separate from delete by using a
read-lock.

\subsubsection{Arrays}
% Advantages
Advantages of using arrays are primarily cache-efficiency, since the
elements can be stored in one consecutive memory segment.
However.. shared cache-lines (ACM article) ... might be a problem with parallelism?

% Disadvantages
When using arrays, however, a tradeoff between efficient insertions or
efficient searches will have to be made. A sorted array is the most compressed
logarithmic time searchable representation of the elements. This makes for
cache- and time-efficient searching of the buckets, while increasing the
insertion cost to become worst case linear in the bucket size, with bigger
constant factors from reallocations.

% Space consumption
If dynamic arrays are used, a 2-approximation can be maintained, meaning the
overhead will be within a factor 2 of the elements' total size, having no
extra pointers to maintain.

Another approach is to allocate the full bucket capacity, possibly creating a
factor B waste.

% Time bounds
This means that either insertion or searching will be linear in the bucket
size, and therefore define the limit on the desired bucket size.

\begin{table}[h!]
    \centering
    \begin{tabular}[here]{ l || c | c | c}
        Operation & Average case & Worst case & Requires locking  \\ \hline
        Insert    & $O(1)$ & $O(1)$ & E \\ \hline
        Delete    & $O(n)$ & $O(n)$ & E \\ \hline
        Find      & $O(n)$ & $O(n)$ & C  \\ \hline
    \end{tabular}
    \caption{Time bounds for the bucket operations using unsorted static
    arrays, $n$ denoting number of elements in the bucket. Insertion might be
    linear because of dynamic resizing of the array. Locking terms use E for
        exclusive and C for concurrent.}
    \label{tab:bounds:unsortedarray}
\end{table}
\begin{table}[h!]
    \centering
    \begin{tabular}[here]{ l || c | c | c}
        Operation & Average case & Worst case & Requires locking  \\ \hline
        Insert    & $O(1)$ & $O(n)$ & E \\ \hline
        Delete    & $O(n)$ & $O(n)$ & E \\ \hline
        Find      & $O(n)$ & $O(n)$ & C  \\ \hline
    \end{tabular}
    \caption{Time bounds for the bucket operations using unsorted dynamic
    arrays, $n$ denoting number of elements in the bucket. Insertion might be
    linear because of dynamic resizing of the array. Locking terms use E for
        exclusive and C for concurrent.}
    \label{tab:bounds:unsortedarray}
\end{table}
\begin{table}[h!]
    \centering
    \begin{tabular}[here]{ l || c | c | c}
        Operation & Average case & Worst case & Requires locking  \\ \hline
        Insert    & $O(n)$ & $O(n)$           & E \\ \hline
        Delete    & $O(n)$ & $O(n)$           & E \\ \hline
        Find      & $O(\log n)$ & $O(\log n)$ & C  \\ \hline
    \end{tabular}
    \caption{Time bounds for the bucket operations using binary search trees,
    $n$ denoting number of elements in the bucket. Locking terms use E for
        exclusive and C for concurrent.}
    \label{tab:bounds:sortedarray}
\end{table}


\subsubsection{Hash tables}
%Advantages
The idea of using hash tables as an intermediate step is dubbed a HAT-trie,
and using bit-wise hashing creates sub-buckets, allowing the buckets
as a whole to become very large without becoming inefficient to access.

%Disadvantages
This presents the issue of creating an efficient hashing function with
low collision rate. That is, finding the ballance between having $B$
hash containers, which is essentially just another level of nodes, and
the number of collisions resulting in skewed distributions of values amongst
the sub-buckets. 

This concept is further covered in Zobel et al. 2005, which covers cache conscious
hash tables. This means that the buckets can be very large, reducing the number of
nodes created in the trie over all, and with a cache-efficient bucket structure
the overall outcome should be very fast.


% Space consumption
Hash tables have a relatively large overhead of lookup tables and stuff...
\fxnote{TODO Look into hash tables!}

%Time bounds
\begin{table}[h!]
    \centering
    \begin{tabular}[here]{ l || c | c | c}
        Operation & Average case & Worst case & Requires locking  \\ \hline
        Insert    & $O(\log n)$ & $O(\log n)$ & E \\ \hline
        Delete    & $O(\log n)$ & $O(\log n)$ & E \\ \hline
        Find      & $O(\log n)$ & $O(\log n)$ & C \\ \hline
    \end{tabular}
    \caption{Time bounds for the bucket operations using hashmaps,
    $n$ denoting number of elements in the bucket. Locking terms use E for
        exclusive and C for concurrent.}

    \label{tab:bounds:hashmap}
\end{table}

\clearpage
\section{STL conformity}
Using the STL containers for reference, the implementation will have to include
a series of functions depending on the chosen container type.

\subsection{STL::(Multi-)Map}
For the implementation to conform to the map definition, the following operators
must be available:
\begin{itemize}
    \item Modifiers
        \begin{itemize}
        \item insert, erase, swap, clear
        \end{itemize}
    \item Operations
        \begin{itemize}
        \item find, count, lower\_bound - successor of x,
            upper\_bound - predecessor of x, equal\_range - for maps == find
        \end{itemize}
\end{itemize}
The swap procedure is trivially implemented by swapping the root node, and clear
is implemented by calling delete on the root, and creating a new one.

\subsection{Implementing Erase}
If the trie is considered an immutable set, this method is not required, and as such
any additions required in this phase can be left out for added performance.

In order to implement {\keyword erase}, the nodes need to be able to determine how many
children are active. If a remove call results in an an empty node, it is removed.

Using the added {\keyword size} variable this is easily determined. However,
the same can be determined using the {\keyword max}- and {\keyword min}
pointers needed for the {\keyword iterator} in previous section.

The {\keyword erase} function is implemented naively, meaning that contractions
are only performed when a bucket is deleted because it's empty. The same goes
for nodes. This means that, if all but one element is erased from a bucket,
the entire trie path will be maintained.

This fits perfectly with the initial description from \cite{Nash2008}, that
the remove procedure is merely the insertion procedure in reverse. One could argue
that this would also imply a reverse bursting operation. This would, however,
require the nodes to know the number of elements in their respective subtree,
in order to determine if the subtree can be reduced to a bucket.

\subsection{Implementing iterators}
In order to be able to iterate over the elements, each bucket will need
to have a {\keyword left}- and {\keyword right} pointer, thus making a
doubly-linked list, or single-linked if only one-way iterators are to
be implemented.


The implementation of iteration could be done in several ways, 
Random access iterators are impossible in a pointer-based structure like this,
or at the very least not efficiently implementable. An option could be using known
sizes of subtrees, and moving down the trie using these offsets, to determine the
correct branch. This would, however, require maintaining these counters on every
insertion and removal, and would still require $O(h)$ time.

Bi-directional iterators are very possible, using the implemented doubly-linked
list of buckets. As such, constant-time access is possible using the head or tail
pointers of the burst trie container, and from there, the buckets can be iterated
linearly following the {\keyword next} or {\keyword prev} pointers. This implementation
therefore adds 2 pointers to each buckets' memory footprint, and some maintenance in the
bucket insert and remove methods.

A tree-walk iterator is another option, requiring no additional bookkeeping in the structure
itself, but it will require a stack of nodes, and allow only iteration from the first or the last element.
This is the iterator type implemented for the binary tree buckets. This kind of iterator requires
a stack of $O(h)$ node pointers, and is therefore more expensive in use than the linkedlist approach.
A combination of tree-walk and linked-lists is another possibility, allowing the tree-walk iterator
to avoid scanning the entire children array.

\begin{table}[h!]
    \centering
    \begin{tabular}[here]{ l | c | c | c | c}
        Method      & end() & Next()      & Memory cost (B) & Memory (use)(B) \\ \hline
        Linkedlist  & $O(1)$        & $O(1 + b) $ & $8\cdot2B$      & $O(1)$ \\ \hline
        Tree-walk 1 & $O(1)$        & $O(hc + b)$ & $0$             & $O(h)$ \\ \hline
        Tree-walk 2 & $O(1)$        & $O(hc + b)$ & $8\cdot2B$      & $O(h)$ \\ \hline
    \end{tabular}
    \caption{The possible options for iterator implementation, their added memory cost
        to the structure, and their memory consumption during use. Here $c$ denotes
        the size of the children array, $b$ denotes the bucket size and $h$ the height
        of the trie.}

    \label{tab:bounds:linkedlist}
\end{table}

The iterator needs to find any predecessor/successor bucket, when maintaining
the linkedlist, these operations are equivalent to the STL defined \\lower\_bound
and upper\_bound operators, on a per-bucket level. As such, the {\keyword
upper/lower\_bound} operations only need to scan the contents of the found
bucket.

The predecessor and successor operations are only called when bursting a bucket,
which means the parent node is locked on invocation. What remains is to reason
that the desired pointers can always be found in the local node subtree,
which leaves the following cases:
\begin{enumerate}
    \item The node is empty on creation of the bucket. The bucket is
    created using the pointers given to the insert procedure, defaulting to
    {\keyword NULL}, which should only occur on the first insertion into the trie.
    \item The node has children. Predecessor/successor can go through
    the children array until finding a used slot. This bucket is now used as
    reference, inserting the new bucket in-between it, and the left (for
    predecessor right) bucket, using its pointers. If the predecessor/successor
    call encounters a node instead of a bucket, the call is made recursively,
    with a new offset (maximum node index for predecessor, and minimum for successor),
    until finding a bucket.
    \item  What remains is to show that the list is maintained on bursting.
    This is done using the bursting buckets' left and right pointers for insertion
    parameters for the first value, creating a bucket in the new node, letting the
    remaining insertions use {\keyword NULL} pointers, and relying on obtaining the
    correct left and right pointers from case 2.
\end{enumerate}

This leaves open how to implement {\keyword predecessor} and {\keyword successor}.
In order to not go through the entire children array in the search for a
{\keyword successor} or {\keyword predecessor}, each node keeps a max and min child pointer,
allowing constant access to the neighboring nodes' first and last elements
relevant for successor and predecessor search. Predecessor thus becomes a
linear search of the current node, followed by at most $O(h)$ accesses to
the {\keyword min} or {\keyword max} children.

In terms of parallelism, an added locking of the buckets is required.
If the successor bucket is always locked before the predecessor, deadlock cannot occur
from insertions into the non-circular linked-list of buckets.

The linked-list pointers are only accessed when creating buckets or iterating
the list, and as such, separate locking could be used for these, to increase
the level of potential parallelism.

\section{Parallelisation}
The primary objective is to avoid data corruption, while allowing several
threads to access the structure at once. For this there are a few schemes with
their respective benefits and drawbacks. Some allow only very little or no
parallelism, others allow conditional parallelism, say for instance having
concurrent reads, exclusive writes, also known as CREW.

\subsection{A locking approach}
A CREW model is easily implemented using a global lock on the structure, which
together with a reference-counter for ongoing searches will allow for the
desired behavior. That is, the searches check the lock state before entering,
and if the structure is not locked, they increase the reference counter,
setting the structure in read-mode.


If it is considered acceptable to have greater overhead from the locks, each
branch could have its own locking, such that only the branch being altered
leaves searches in a waiting state. As such, each level in the trie incurs a
constant overhead, checking the locks before accessing.

The algorithm will unfortunately be serial until the lookup in the bucket
structure, which can, depending on the chosen bucket structure, be
paralellised. This means that $n$ searches can be performed in $O(l+b)$ time,
where $b$ is variable on chosen method for bucket search, but that if
simultanious insertions are performed, the insertion can possibly delay all
other operations.



\subsection{Avoiding locks}
Insertions consist of two phases, the bucket insertion, and the possible
bursting. Bursting can be made dynamically parallel up to a factor of $O(b)$ by
using one thread for each element in the queue, and relying on the recursive
structure of the trie. This means bursting can theoretically be done in
$\omega(1)$ using $b$ processors, but the upper bound depends on the
distribution of the nodes. That is, the only case where the elements will cause
further bursting is if they are identical on the next part of the key, which
has a $\frac{1}{\alpha^b}$ probability.

What if the buckets { grew exponentially} - that would let the bursting be
guaranteed constant, since $b$ elements cannot possibly cause a burst of a
$b*a$ capacity container, with $a > 1$.


I postulate, however, that the structure can be made parallel using a lock-free
bursting scheme, assuming that the bucket container has a series of properties.

If we are to allow searches to proceed unaffected during bursting, the bucket
container needs to be kept active until the nodes have all been inserted into the
newly created node, upon which the pointers are swapped in an atomic operation.
Thus, the bursting thread redistributes the elements in a ghost node, which is
only visible to the trie upon this pointer switch. There might, however be
active searches in the old bucket, which means the bucket needs a reference
counter, which the bursting thread will then have to wait for to become zero.
Then the bucket is removed. The bucket is hereby used as a queue for the
insertion into the new node, and can be searched during this time, if the
elements are copied instead of removed.

The interesting part is when another insertion happens into the same bucket,
which is currently being burst. The bucket structure is assigned an insertion
queue, for use during bursting, since there is no point in making sorted
insertions, when the elements are to be redistributed next.

If the trivial option is chosen, the queue
operates in a FIFO manner during bursting, meaning that the other insertion
happens by simply appending the element to the queues tail in constant time.

Another option could be to allow the container itself to violate its structure
during this phase, to allow atomic insertion.
This requires a buffer to be available in the buckets, for instance by making
sure the bucket is expanded "x" beyond the bucket capacity upon insertion of
the violating element.

The bucket will then need to be flagged, for use with searches, if the order of
the elements is violated on insertion during bursting. A linear search approach
is the forced. The question is whether reusing the bucket is actually the best
solution, or whether a special insertion queue should be used, for instance in
the form of a linked list, which can easily be maintained atomically. Searches
will then have to look at both the linked list and the bucket, but this could
result in $O(\log b + i)$ time instead of $O(b+i)$.

{ Note:} If any  of the STL containers are used within the trie, they will
have to be locked on writing, since the STL containers are not thread-safe by
design.

\subsubsection{Linked lists}
A linked list can be maintained using atomic pointer operations, allowing
sorted insertions without relying on locking.
Linked lists are also easily expanded in the way described to be required during
bursting, by appending to the tail of the list, and having the list keep a
pointer to know which of them are in violation, and thereby may require a
full linear lookup, instead of searching only till reaching elements that are
too large.

\subsubsection{Arrays}
It is not possible to avoid locks entirely if the arrays are to be dynamically
expanded during insertions. That is, there is a possibility of a dynamic
reallocation of the array on each insertion.

If the values are sorted as well, almost every insertion will require moving
other elements in the array, and since the array might become invalid in the
mean time, locking is required.

\subsubsection{Hash tables}
If the buckets are of the HAT-trie type, a logical distribution of workers
can be made onto the respective sub buckets, which can then again be burst in
the same way as the aforementioned simple buckets, but will require extra checks
because of the extended structure. The last worker to move all its values will
therefore have to remove all the other buckets, and the hash table, if searches
are to continue using the structure untill the bursting has completed.

\subsubsection{Parallel algorithms}
On a theoretical level, parallelism is investigated in accordance with Amdahls
law, analyzing each part of the procedure for possible parallelisation.

Following the definition of parallel algorithms, the burst trie algorithms of
page \pageref{alg:bt_insert} are equivalent to the sequential versions, but
with parallelised loops where possible.


\begin{algorithm}[H]
    \caption{\FuncSty{Insert(}$k$\FuncSty{)}}
    \SetKwFunction{Burst}{burst}

    $Node \leftarrow$  root node\;
    \tcp{Move down until reaching bucket reference}
    \ForEach{char $c$ in $k$}{
        \If{$Node$ has child $c$}{
            $Node \leftarrow Node[c]$\;
        }
    }
    \tcp{Node either has a bucket at $c$ or one is created}
    \eIf{$Node$ has bucket $B[c]$}{
        \emph{Insert remaining $k$ in $B$}\;
        \If{$B$ is full}{
            $Node[c] \leftarrow \Burst(B)$\;
        }
    }{
        $b \leftarrow $\emph{Create bucket and insert $k$}\;
    }
\end{algorithm}


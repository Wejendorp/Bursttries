\chapter{Implementation}
By first establishing the regular trie methods, a basis is made for evaluating
the modifications required by the other types. All used structures are
implemented from scratch, unless otherwise noted.

In the analysis of the various containers and the general trie, the following
notation will be used:
% Time bounds
\begin{table}[h!]
    \centering
    \begin{tabular}[here]{ l l}
        Notation & Meaning \\ \hline
        n       & The number of elements.\\
        $\alpha$& The alphabet size, or node size.\\
        b       & The bucket size.\\
        h       & The (maximum-)height of the trie. \\
    \end{tabular}
    \caption{Table of the analysis notation}
    \label{tab:notation}
\end{table}

\section{The trie}
The basic trie creates one node for each unique suffix character of the input string,
and as such will always have the height of the longest string inserted.
The {\keyword insert} method builts the trie using the following algorithm:

\begin{algorithm}[H]
    \caption{Generic trie \FuncSty{Insert(key k)}}
    \label{alg:gt_insert}

    $Node \leftarrow$  root node\;
    \ForEach{char $c$ in $k$}{
        \eIf{$Node$ has child $c$}{
            $Node \leftarrow Node[c]$\;
        }{ %else
            $Node[c] \leftarrow$ \emph{new $Node$}\;
            $Node \leftarrow Node[c]$\;
        }
    }
    \emph{Set end of string flag on $Node$}\;
\end{algorithm}

Using this as the basis for the insertion and deletion methods, the trie is
built and searched by reading one character at a time. So just like an
insertion might create a deep path down the trie, a deletion will have to
remove dead ends upon removing an entry. A depth-first approach is taken
with deletions. The recursive call returns with a value, determining if
changes might be needed at the higher level.

If the trie is used to store (key,value) pairs, the end of string flag is
simply a pointer to the value. The trie operations depend solely on the height,
and insertions, searches and removals are all $O(h)$.

\subsection{Burst trie}
The burst trie uses the same basic methods as outlined above. The child check
"c" is extended to cover two kinds of children instead of just nodes. The
descent condition of the regular trie is to move down until end of string, while
the burst trie will also stop when a bucket is found or created.
The default action for a non-existing child is to create a bucket. The unread part
of the key is then inserted into the bucket, and the bucket is burst if needed.

\begin{algorithm}[H]
    \caption{Burst trie \FuncSty{Insert(key k, value v)}}
    \label{alg:bt_insert}

    \SetKwFunction{Burst}{burst}

    $Node \leftarrow$  root node\;
    \tcp{Move down until reaching bucket reference}
    \ForEach{char $c$ in $k$}{
        \eIf{$Node$ has child-node $c$}{
            $Node \leftarrow Node[c]$\;
        }{
            \eIf{$Node[c]$ is bucket $B$}{
                \emph{Insert remaining $k,v$ in $B$}\;
                \If{$B$ is full}{
                    $Node[c] \leftarrow \Burst(B)$\;
                }
            }{
                $b \leftarrow $\emph{Create bucket and insert remaining $k,v$}\;
            }
            \Return\;
        }
    }
    $Node.value \leftarrow v$\;
\end{algorithm}

Because of the added bursting and bucket insertion, the time complexity of
insertion becomes $O(h+I_b\cdot b)$, where $I_b$ denotes insertion time of the
bucket. Bursting is linear in bucket size, since insertions into an empty node
are constant, and the bucket has at most b elements.
Assuming linear bucket insertion time and only one burst, the worst case
insertion bound becomes $O(h+b) + \sum_{n=1}^b n  = O(h+b^2)$. This only occurs
of all $b$ elements have the same next character, which has a probability of
$\frac{1}{\alpha^b}$ in a uniformly distributed dataset.
(see table \ref{tab:notation} for notation)

Deletions are equivalent to insertions in that removing a key may cause
removal/addition of a bucket, and subsequently nodes. The method is equivalent
to that of the basic trie, or the insertion algorithm in reverse.

The aforementioned bursting is simply an iteration over the contents of the
bucket in order to insert them in a newly created child node, which is to
take the buckets' place. This extends the trie by another level, and redistributes
the keys into up to $\alpha$ new buckets in $O(b \cdot I_b)$, where $I_b$ denotes
insertion time into a bucket.

\begin{algorithm}[H]
    \caption{Burst trie \FuncSty{Burst(bucket b)}}
    \label{alg:bt_burst}
    \SetKwFunction{Insert}{insert}
    $Node \leftarrow$ new $Node$\;
    \ForEach{value $k,v$ in $b$}{
        $Node.$\Insert($k,v$)
    }
\end{algorithm}

\begin{algorithm}[H]
    \caption{Burst trie \FuncSty{Find(key k)}}
    \label{alg:bt_search}

    $Node \leftarrow$  root node\;
    \ForEach{char $c$ in $k$}{
        \eIf{$Node$ has child $c$}{
            $Node \leftarrow Node[c]$\;
        }{ %else
            \eIf{$Node$ has bucket $B[c]$}{
                \emph{Lookup remaining $k$ in $B[c]$}\;
            }{
                \Return{Nothing}\;
            }
        }
    }
    \Return $Node.value$\;
\end{algorithm}

The trie itself can be implemented in various ways. The original paper by Heinz
et. al \cite{Heinz:2002} uses arrays, since the goal of the trie is to avoid
element comparisons as much as possible. This allows the branching to be a
constant-time operation, but at the cost of $\alpha$ potentially unused slots
allocated ($\alpha$ being the node or alphabet size). A slot, being a pointer,
uses 8 bytes of space on a 64-bit machine.
For the basic implementation, the nodes need only contain this array of size
$((\alpha+1) \cdot 8)$ bytes, where the $+1$ is a pointer to the value type.


\section{Bucket structures}
% Advantages
% Disadvantages
% Time bounds
Depending on the chosen bucket structure, insertions, deletions and searches
have different time complexities, and allow different bucket sizes without
becoming inefficient. An added dimension to this research when compared to
previous attempts is the changes needed to make each container threadsafe.

\subsection{Linked lists}
% Advantages
Linked lists are easy to maintain, setting two or four pointers respectively, for
single- and doubly-linked list insertions and deletions.
If the linked list is maintained with a {\keyword head} and {\keyword tail} pointer,
and kept single-linked, insertions and deletions should be possible
using a simple {\keyword AO\_compare\_and\_swap} pointer-swap.

% Disadvantages
A linked-list is known for its bad cache locality, which makes it slow in
searches and iterations when compared to an array implementation, despite
having the same time bounds. A linked list is a pointer-based bucket, making
random-acccess impossible, resulting in limited options for searching and
sorting.

% Space consumption
The space consumption of linked lists is equivalent to that of a binary tree,
if a doubly-linked list is used, or one pointer less per entry for a
single-linked list.

% Time bounds
\begin{table}[h!]
    \centering
    \begin{tabular}[here]{ r || c | c | c}
        Operation & Average case & Worst case & Requires locking  \\ \hline
        Insert    & $O(1)$ & $O(1)$   & E  \\ \hline
        Delete    & $O(n)$ & $O(n)$   & E \\ \hline
        Find      & $O(n)$ & $O(n)$   & C  \\ \hline
    \end{tabular}
    \caption{Time bounds for the bucket operations using unsorted linked lists,
    with a basic locking scheme. $n$ denotes the number of elements in the
    list. Locking terms use E for exclusive and C for concurrent.}
    \label{tab:bounds:linked list}
\end{table}

\subsection{Binary search tree}
% Advantages
The unbalanced binary search trees allow for an (average case) logarithmic
insertion, search and deletion structure, which is a known lower bound. So, the
binary search trees are theoretically optimal in the average case, while having
little space overhead.

% Disadvantages
However, the binary search trees were found to be inefficient due to
caching\cite{Nash:2008}, being a pointer-based bucket structure. A basic binary
search tree has no guarantees on the height, which is based on the insertion
order. This problem is persistent through bursting, since the insertion order
determines the order of tree-traversal during bursting. The elements will obtain
a new ordering through leading letter removal, which might make an optimal ordering
on one level suboptimal in the next.

%Space consumption
The binary search trees will only use 2 pointers per element, and
has no overhead from preallocation. 

% Time bounds
\begin{table}[h!]
    \centering
    \begin{tabular}[here]{ l || c | c | c}
        Operation & Average case & Worst case & Requires locking  \\ \hline
        Insert    & $O(\log n)$ & $O(n)$ & E \\ \hline
        Delete    & $O(\log n)$ & $O(n)$ & E \\ \hline
        Find      & $O(\log n)$ & $O(n)$ & C \\ \hline
    \end{tabular}
    \caption{Time bounds for the bucket operations using unbalanced binary
    search trees, $n$ denoting number of elements in the bucket. Locking terms
    use E for exclusive and C for concurrent.}
    \label{tab:bounds:bst}
\end{table}


For this project, the \STL map implementation is used for testing against the
self-balanced binary tree, which is the underlying implementation in most
distributions. An added space overhead when compared to the unbalanced tree is
expected.

%Time bounds
\begin{table}[h!]
    \centering
    \begin{tabular}[here]{ l || c | c | c}
        Operation & Average case & Worst case & Requires locking  \\ \hline
        Insert    & $O(\log n)$  & $O(\log n)$ & E \\ \hline
        Delete    & $O(\log n)$  & $O(\log n)$ & E \\ \hline
        Find      & $O(\log n)$  & $O(\log n)$ & C \\ \hline
    \end{tabular}
    \caption{Time bounds for the bucket operations using hashmaps, specifically
    the \STL Map implementation. $n$ denoting number of elements in the
    bucket. Locking terms use E for exclusive and C for concurrent.}

    \label{tab:bounds:map}
\end{table}

\subsection{Arrays}
% Advantages
Advantages of using arrays are primarily cache-efficiency, since the
elements can be stored in one consecutive memory segment.

% Disadvantages
When using arrays, however, a tradeoff between efficient insertions or
efficient searches will have to be made. A sorted array is the most compressed
logarithmic time searchable representation of the elements. This makes for
cache- and time-efficient searching of the buckets, while increasing the
insertion cost to become worst case linear in the bucket size, with bigger
constant factors from reallocations.

% Space consumption
If dynamic arrays are used, the array will have to be resized during some
insertions. One approach could be to use an exact-fit array, but this would
come at the cost of reallocating the array on every insertion. The space
allocated can be maintained at a factor 4 of the actual load requiring only a
logarithmic reallocation count. The overhead will be within a
factor 4 of the elements' total size, since the array has no extra pointers to
maintain. This is done by simple doubling, leading to an amortized constant
time for reallocations. 

Another approach is to allocate the full bucket capacity (a static array),
possibly creating a factor $b$ waste if the bucket load is low.

% Time bounds
This means that either insertion or searching will be linear in the bucket
size, and therefore define the limit on the desired bucket size.

\begin{table}[h!]
    \centering
    \begin{tabular}[here]{ l || c | c | c}
        Operation & Average case & Worst case & Requires locking  \\ \hline
        Insert    & $O(1)$ & $O(1)$ & E \\ \hline
        Delete    & $O(n)$ & $O(n)$ & E \\ \hline
        Find      & $O(n)$ & $O(n)$ & C  \\ \hline
    \end{tabular}
    \caption{Time bounds for the bucket operations using unsorted static
    arrays, $n$ denoting number of elements in the bucket. Insertion might be
    linear because of dynamic resizing of the array. The cost of the worst case
    constant insertion is one of a potentially large memory waste. Locking
    terms use E for exclusive and C for concurrent.}
    \label{tab:bounds:unsortedarray}
\end{table}
\begin{table}[h!]
    \centering
    \begin{tabular}[here]{ l || c | c | c}
        Operation & Average case & Worst case & Requires locking  \\ \hline
        Insert    & $O(1) amortized$ & $O(1)$ amortized & E \\ \hline
        Delete    & $O(n)$ & $O(n)$ & E \\ \hline
        Find      & $O(n)$ & $O(n)$ & C  \\ \hline
    \end{tabular}
    \caption{Time bounds for the bucket operations using unsorted dynamic
    arrays, $n$ denoting number of elements in the bucket. Insertion might be
    linear because of dynamic resizing of the array. Locking terms use E for
        exclusive and C for concurrent.}
    \label{tab:bounds:unsorteddynarray}
\end{table}
\begin{table}[h!]
    \centering
    \begin{tabular}[here]{ l || c | c | c}
        Operation & Average case & Worst case & Requires locking  \\ \hline
        Insert    & $O(n)$  & $O(n)$           & E \\ \hline
        Delete    & $O(n)$  & $O(n)$           & E \\ \hline
        Find      & $O(\log n)$ & $O(\log n)$ & C  \\ \hline
    \end{tabular}
    \caption{Time bounds for the bucket operations using sorted dynamic arrays,
    $n$ denoting number of elements in the bucket. Locking terms use E for
        exclusive and C for concurrent.}
    \label{tab:bounds:sortedarray}
\end{table}

% Summary?

\section{\STL  conformity}
Using the STL containers for reference, the implementation will have to include
a series of functions depending on the chosen container type.

\subsection{STL::(Multi-)Map}
For the implementation to conform to the map definition, the following operators
must be available:
\begin{itemize}
    \item Modifiers: {\keyword insert}, {\keyword erase}, {\keyword swap},
        {\keyword clear}
    \item Operations: {\keyword find}, {\keyword count}, {\keyword lower\_bound},
        {\keyword upper\_bound}
    \item Iterators: {\keyword begin}, {\keyword end}, {\keyword rbegin},
        {\keyword rend}
\end{itemize}
The {\keyword swap} procedure is trivially implemented by swapping the root
node, and {\keyword clear} is implemented by calling {\keyword delete} on the
root, and creating a new one. The {\keyword insert} and {\keyword find}
procedures have been covered.

\subsection{Implementing Erase}
If the trie is considered an immutable set, this method is not required, and as such
any additions required in this phase can be left out for added performance.

In order to implement {\keyword erase}, the nodes need to be able to determine how many
children are active. If a remove call results in an empty node, it is removed.

Using an added {\keyword size} variable this is easily determined by evaluating the
return value of the remove call if the current node contains only one child.
However, the same can be determined using the {\keyword max}- and {\keyword
min} pointers (or bitvector) needed for the {\keyword iterator} in iterator
section.

The {\keyword erase} function is implemented naively, meaning that contractions
are only performed when a bucket is deleted because it is empty. The same goes
for nodes. This means that, if all but one element is erased from a bucket,
the entire trie path will be maintained.

This fits perfectly with the initial description from \cite{Nash:2008}, that
the remove procedure is merely the insertion procedure in reverse. One could argue
that this would also imply a reverse bursting operation. This would, however,
require the nodes to know the number of elements in their respective subtree,
in order to determine if the subtree can be reduced to a bucket.
This is not covered in this project, however.

%For more information on contraction heuristics in burst tries, see 

\subsection{Implementing iterators}
In order to be able to iterate over the elements, each bucket will need
to have a {\keyword left}- and {\keyword right} pointer, thus making a
doubly-linked list of the buckets, or single-linked if only one-way iterators
are to be implemented.

The implementation of iteration could be done in several ways. Random access
iterators are impossible in a pointer-based structure like this, or at the very
least not efficiently implementable. An option could be using known sizes of
subtrees, and moving down the trie using these offsets to determine the correct
branch. This would, however, require maintaining these counters on every
insertion and removal, and would still require $O(h)$ time.

A tree-walk iterator is another option, requiring no additional bookkeeping in
the structure itself, but it will require a stack of nodes, and allow only
iteration from the first or the last element. This is the iterator type
implemented for the binary tree buckets. This kind of iterator requires a stack
of $O(h)$ node pointers, and is therefore more expensive in use than the linked
list approach.

\begin{table}[h!]
    \centering
    \begin{tabular}[here]{ l | c | c | c | c}
        Method      & begin()  & Next()           & Memory cost (B) & Memory (use)(B) \\ \hline
        Linked list  & $O(1)$ & $O(1 + b) $        & $8\cdot2B$      & $O(1)$ \\
        Tree-walk & $O(1)$ & $O(h \cdot c + b)$   & $0$             & $O(h)$ \\ \hline
    \end{tabular}
    \caption{The possible options for iterator implementation, their added
    memory cost to the structure, and their memory consumption during use. Here
    $c$ denotes the size of the children array, $b$ denotes the bucket size and
    $h$ the height of the trie.}

    \label{tab:stats:iterator}
\end{table}

Bi-directional iterators are very possible, using an implementation of a
doubly-linked list of buckets. As such, constant-time access is possible using
the {\keyword head} or {\keyword tail} pointers of the burst trie container,
and from there, the buckets can be iterated linearly following the {\keyword
next} or {\keyword prev} pointers. This implementation therefore adds 2
pointers to each buckets' memory footprint, and some maintenance in the bucket
insert and remove methods.

The iterator needs to find any predecessor/successor bucket, when maintaining
the linked list, these operations are equivalent to the STL defined {\keyword
lower\_bound} and {\keyword upper\_bound} operators, on a per-bucket level. As such, the
{\keyword upper/lower\_bound} operations only need to scan the contents of the
found bucket, after performing a {\keyword find} of the key, then using the
linked-list to obtain predecessor/successor bucket if a lower/higher element is
not found.

The predecessor and successor operations are only called when bursting a bucket,
which means the parent node is locked on invocation. What remains is to reason
that the desired pointers can always be found in the local node subtree,
which leaves the following cases:
\begin{enumerate}
    \item The node is empty on creation of the bucket. The bucket is
    created using the pointers given to the insert procedure, defaulting to
    {\keyword NULL}, which should only occur on the first insertion into the trie.
    \item The node has children. Predecessor/successor can go through
    the children array until finding a used slot. This bucket is now used as
    reference, inserting the new bucket in-between it, and the left (for
    predecessor right) bucket, using its pointers. If the predecessor/successor
    call encounters a node instead of a bucket, the call is made recursively,
    with a new offset (maximum node index for predecessor, and minimum for successor),
    until finding a bucket.
    \item  What remains is to show that the list is maintained on bursting.
    This is done using the bursting buckets' left and right pointers for insertion
    parameters for the first value, creating a bucket in the new node, letting the
    remaining insertions use {\keyword NULL} pointers, and relying on obtaining the
    correct left and right pointers from case 2.
\end{enumerate}

This leaves open how to implement {\keyword predecessor} and {\keyword
successor}. In order not to go through the entire children array in the search
for a {\keyword successor} or {\keyword predecessor}, each node keeps a
{\keyword max} and {\keyword min} child pointer, allowing constant access to
the neighboring nodes' first and last elements relevant for successor and
predecessor search. Predecessor thus becomes a linear search of the current
node, followed by at most $O(h)$ accesses to the {\keyword min} or {\keyword
max} children, resulting in an $O(h+b)$ operation.

A more efficient solution is to use a bitvector with the same size as the
children array, allowing constant-time updates of the {\keyword max} and
{\keyword min} pointers, which are otherwise dependent on a linear search of
the array. For this purpose there exists an assembly instruction to find the
first 1 bit in a DWORD. If these bitvectors were used instead of {\keyword max}
and {\keyword min}, the factor $b$ of updating these can be made constant.


\section{Parallelisation}
The primary objective is to avoid data corruption, while allowing several
threads to access the structure at once. For this, there are a few schemes with
their respective benefits and drawbacks. Some allow only very little or no
parallelism, others allow conditional parallelism, say for instance having
concurrent reads, exclusive writes, also known as CREW.

\subsection{A locking approach}
A CREW model is easily implemented using a global lock on the structure, which
together with a reference-counter for ongoing searches will allow for the
desired behavior. That is, the searches check the lock state before entering,
and if the structure is not locked, they increase the reference counter,
setting the structure in read-mode. This is the simplest form of locking.
Since the \STL containers allow only concurrent reads, not writes, exclusive
write-access is needed, limiting the insertions to be sequential.

A test run with this type of locking is given in Figure \ref{fig:globallock}
on Page \pageref{fig:globallock}.
It shows no gains during insertions over the sequential (1-thread) run by
adding more threads. However, this solution allows the structure to remain
functional in a parallel environment, which is the first step in obtaining
parallelism.


If it is considered acceptable to have greater overhead from the locks, each
node can have its own locking, such that only the branch being altered
leaves searches in a waiting state. As such, each level in the trie incurs a
constant overhead, checking the locks before accessing.

The algorithm will unfortunately be serial until the lookup in the bucket
structure, which can, depending on the chosen bucket structure, be
paralellised. This means that $n$ searches can be performed in $O(h+S_b)$ time,
where $S_b$ depends on the chosen method for bucket search, but that if
simultanious insertions are performed, the insertion can possibly delay all
other operations. 

A node is locked during insertion, since changes can be made to the children
array. The locking is then moved down through the trie, obtaining a lock on
the child before releasing its own lock. The current node or bucket is
therefore always locked.

The update of the three buckets will need to happen in one
step to avoid race conditions. If the successor bucket is always locked before
the predecessor, deadlock cannot occur from insertions into the non-circular
linked-list of buckets. This is under the assumption that these pointers are
only locked in this way. The linked list pointers are only accessed when
creating buckets or iterating the list, and as such, separate locking could be
used for these to increase the level of potential parallelism. The general
approach with iterators and insertions, however, is to invalidate all iterators
on insertion. As such, the programmer is responsible for not using an iterator,
during insertion as the behavior is undefined.

Removals are implemented in a way that very much resembles the insertions. The
locking passes down, following the current node until reaching a node or bucket
of size one (one child if in a node, one element if in a bucket). When this
occurs no unlocking is done until the recursive remove call returns, allowing
the node to remove the subtree if needed. The removals are therefore even less
parallel than the insertions.

\subsection{Avoiding locks}
% Still interesting!
The greatest bottleneck of the trie is the locking at the root, allowing only
one writer at a time. The locking allows more writers for each level, but they
must all first go through the root. To avoid exclusive locking of the nodes,
atomic operations can be used in combination with contention management.
As a principle in the following, concurrent mixed operations are undefined.

The proposed solution is designed for building the dataset quickly in
parallel, then performing other operations. With this in mind, bursting can be
changed to insert an empty node immediately, allowing other threads to insert
into this node without knowing that a bursting is in progress. As such, a
bursting delays only the thread doing the bursting. An unlucky thread, however,
might burst one bucket, only to trigger a burst of another, and so on. While
highly unlikely, the time between the new node being available to other threads
and inserting an element from the old bucket, the bucket may have been filled.
This does pose a worst case bound of $O(\min(\alpha, b)^h)$, if every insertion
into the child triggers new bursts. 

If updating an element is dependent on the state before the update, the operation
is called data dependent. If the value has changed before the operation can write
its new value, the work is rolled back. The thread then tries again to read the
value and perform the appropriate operation.

Reading an index in the children array is a data dependency for the insert
operation that needs to be performed. If the insertion finds an empty child,
a bucket is created and the CAS (Compare And Swap) is successful if
the child is still empty when the bucket reference is swapped back to the
index. If CAS fails, the orphaned bucket is removed again, and the index is
re-read. In the worst case, this presents only a constant amount of wasted work
per level in the trie since either way a valid bucket is created for the
insertion.

If a bucket is found at the index the burst condition is checked. If a bursting
is required a new node is created; the CAS is successful if the index
still points to the bucket, making it point to the node. If this is done
immediately before redistributing the keys into the node, potentially wasted
work is again only constant. Doing the swap before redistributing the
keys leaves those keys invisible to other threads, which is why any concurrent
search/remove calls are undefined. This guarantees at most a constant
number of operations when creating a new level in the trie. An insertion
therefore takes $O(h+I_b)$, without having to wait for locks.

The worst case for bursting is so so unlikely, that for all practical purposes
an $O(b^2)$ burst time is expected. This becomes the dominant factor for most
insertions, resulting in $O(h+b^2)$ insertion time for any number of concurrent
insertions.

\subsubsection{Assisting}
On a theoretical level, parallelism is investigated in accordance with Amdahls
law, analyzing each part of the procedure for possible parallelisation.

Following the definition of parallel algorithms, the burst trie algorithms of
Page \pageref{alg:bt_insert} are equivalent to the sequential versions, but
with parallelised loops where possible. Each new iteration of the lookup
is dependent on the previous, and can therefore not be parallelised. This
establishes the $O(h)$ factor in the operations despite parallelism.

We postulate that the burst bounds can be reduced by adding assisting logic to
what is invariably the most expensive operation of the trie. Iterating the
bucket during bursting can be done wait-free, assuming that the bucket
container has a wait-free persistent iterator.

Bursting can be made dynamically parallel up to a factor of $O(b)$ by
using one thread for each element in the bucket, and relying on the recursive
structure of the trie. This means bursting can theoretically be done in
$O(1)$ using $b$ processors, but the upper bound depends on the
distribution of the elements.

The assist logic can be implemented by adding a bucket pointer to the
newly created node (a reference to the bursting bucket), and checking whether
this is set when inserting into a node. If it is set, the threads join
in on iterating over the bucket, taking part in the remaining workload.
This means that if $b$ insertions are made into the node
below a bursting bucket, $b$ processors will join. Checking this pointer
done on every level, taking $O(h)$ time, and not adding to the complexity.
This makes it possible to encounter
$O(b)$ work at every level, resulting in a parallelised wait-free
bound of $O(hb)$ for $n$ insertions on $n$ processors.

\subsubsection{The bucket factor}
% BUT BUCKETS SUCK!
If the buckets are still using locks, the algorithm will have a maximum
parallelism factor of the bucket count. The payoff of the assist implementation
is then at most a factor $\alpha$, the bucket count under the new node.

While making these structures lock-free is not the focus of this project, some
bucket types are simpler to implement than others:
\begin{itemize}
\item A linked list can be maintained using atomic pointer operations, allowing
sorted and unsorted insertions without relying on locking. The insertion creates
a new node, loads the neighbor pointers, sets the pointers and checks if they have
changed requiring a new attempt.
\item A binary tree is pointer based like the linked list, and can be
implemented in the same way.
%
\item It is not possible to avoid locks entirely if the arrays are to be
dynamically expanded during insertions. That is, there is a possibility of a
dynamic reallocation of the array on each insertion. If the values are sorted
as well, almost every insertion will require moving other elements in the
array, and since the array is invalid in the meantime, locking is
required. The only possible arrays are therefore static.
\end{itemize}

\chapter{Introduction}

\section{Problem definition}
How can the burst trie be parallelised in a way that allows building and searching
the structure faster than an equivalent serial singlethreaded approach?
What kind of overhead does it create, and does this influence relative
performance between the individual variants of the structure?


\section{Background}
%Define: What is a trie?
A trie is a data structure for storing keys based on their prefixes.
It is also known as a prefix-tree. The idea is equivalent to that of
"most significant digit" (MSD-)radix sort.

At each level in the tree a prefix of the key is read and branches performed
accordingly. A trie working on strings with the ASCII alphabet will thus have a
branching factor of 128. The character that is read corresponds to a branch in
the current node. This branching moves down another level in the trie until
finding the leaf matching the requested key or terminating unsuccesfully. A
trie is as deep as the longest key that it contains, since each level removes a
character from the key. It is this bound that newer trie variants attempt to
improve, without compromising lookup efficiency.

%Define: What is the idea of a burst trie?
Following the radix sort analogy, the regular trie uses radix-sort until
reaching the trivial case, with no boundary for using other sorting methods.
The burst trie on the other hand, uses a limit for employing other methods
based on the element count. In other words a burst trie differs from a regular
trie by compressing the elements into so-called bucket structures when
there are few of them in a particular subtree. Each bucket is then assigned a
capacity. When inserting elements into a burst trie, the structure finds the
corresponding bucket and puts the element in it. If the bucket exceeds its
capacity it "bursts", creating a new parenting node and redistributing the
elements into buckets under it, based on the next part of the prefix. As such,
each bucket is uniquely determined by its prefix (or path in the trie). The
elements in each bucket therefore all have the same prefix, making it possible
to discard this part of the key to save space. This is what makes it possible
for the trie to be more space efficient than an equivalent hashmap, or
    comparison-based tree structure.

% Why are burst tries nice?
Burst tries have been shown to be faster and more space efficient than
compari\-son-based data structures such as B-trees and red-black trees when used
on integer data\cite{Nash:2008}. For strings the burst trie is more efficient
than trees\cite{Heinz:2002} while unlike hash-tables allowing efficient iteration
of sorted keys with minor modifications. These modifications are one of the
focus areas of this project.

The original burst trie uses linked lists for the containers, but this has been
shown to be inefficient in a modern cache hierachy. There has been done research
in different ways to improve the structure, by replacing the bucket structure.
Nash \& Gregg have done work on optimizing the trie itself, using so-called
level- and path-compression\cite{Nash:2008}. More complex tries exist in
the forms of both Judy and Patricia tries. They work by dynamically changing
the bucket types depending on load, and using different metrics to compress
the trie itself, but are more sensitive to skew data distributions.

In an earlier course, the author has done work on the burst trie as an Integer
data structure\cite{Wejendorp:2010} in cooperation with Niels Grathwohl. There,
a simplistic bit-string approach was used to treat integers as bit-strings when
branching in an attempt to implement parts of, and verify the results of Nash
\& Gregg\cite{Nash:2008}. That work is extended in this project in the form of
investigating a wider range of solutions for each addition to the basic trie.

% Multithreading!! - The open problem!
While maintaining optimizations for caching remains important, modern processor
scaling primarily focuses on multithreading, making it increasingly important
to create efficient multi-threaded data structures and algorithms. To the
author's knowledge, no work has been done on parallelising the structure for
modern multi-core processors, as this was left open by Askitis and Sinha in
\cite{Askitis:2010}. The goal of this project is therefore specifically
parallelising the structure in a way that allows insertions and deletions
faster than the sequential implementation, while remaining space
efficient\cite{Askitis:2010}. This means evaluating the sacrifices needed to
make the structure threadsafe; the number of locks, indicator variables and the
like.

Parallelism in general is a very complex issue, with cache alignment having a
potentially large impact on performance, from so-called false sharing
\cite{Shavit:2011}. This phenomenon occurs when two CPUs operate on the same
memory region, but on independent data, and is the unnecessary synchronization
of cache lines between CPUs.

Another area of interest is minimal locking, which will decrease the amount of
syncrhonization between threads. The standard data structures of C++ are all
threadsafe in terms of concurrent reads, but not writes. As such, a simple
read/write lock can be used to wrap the structure, causing forced sequential
writing, while allowing multiple readers. When locks are used, the scheduler
must guarantee that the threads holding the locks are not preempted. Otherwise
the performance of all waiting threads will suffer. A solution to this is to
try and design a lock-free implementation.\cite{Shavit:2011}

What makes lock-free implementation difficult is potential race conditions from
data dependencies. This is attempted to be resolved using a
mixture of so-called atomic operations, in combination with compare-and-swap
(CAS) and contention. Using this method, one operation will always succeed for
every time another fails, and progress is guaranteed regardless of what the
scheduler does\cite{Shavit:2011}.

It is therefore of great interest to investigate ways to make the trie lock-free.
Doing so will also ensure operations not being limited in terms of parallelisation
by the granularity of the locking mechanisms.
